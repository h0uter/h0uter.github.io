<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MLDG reproduction | Wouter J. Meijer </title> <meta name="author" content="Wouter J. Meijer"> <meta name="description" content="Meta Learning for Domain Generalisation enables us to train on photos and then classify drawings."> <meta name="keywords" content="robotics, ai, engineer, academic-website, portfolio-website"> <meta property="og:site_name" content="Wouter J. Meijer"> <meta property="og:type" content="website"> <meta property="og:title" content="Wouter J. Meijer | MLDG reproduction"> <meta property="og:url" content="https://h0uter.github.io/projects/2020-04-20=MLDG-project/"> <meta property="og:description" content="Meta Learning for Domain Generalisation enables us to train on photos and then classify drawings."> <meta property="og:image" content="/assets/preview_image_projects.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="MLDG reproduction"> <meta name="twitter:description" content="Meta Learning for Domain Generalisation enables us to train on photos and then classify drawings."> <meta name="twitter:image" content="/assets/preview_image_projects.png"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://h0uter.github.io/projects/2020-04-20=MLDG-project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Wouter</span> J. Meijer </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">+ </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/tools/">tools</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/resources/">resources</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MLDG reproduction</h1> <p class="post-description">Meta Learning for Domain Generalisation enables us to train on photos and then classify drawings.</p> </header> <article> <h1 id="introduction">Introduction</h1> <hr> <blockquote> <p>Humans are adept at solving tasks under many different conditions. This is partly due to fast adaptation, but also due to a lifetime of encountering new task conditions. This provides the opportunity to develop strategies, which are robust to different task contexts.</p> </blockquote> <blockquote> <p>We would like artificial learning agents to do the same because this would make them much more versatile and perform better ‘out-the-box’.</p> </blockquote> <p>This paper proposes a novel meta learning approach for domain generalisation rather than proposing a specific model suited for DG.</p> <table> <tbody> <tr> <td>Bob [<a href="https://github.com/bobluppes" rel="external nofollow noopener" target="_blank">Github</a>]</td> <td>Wouter [<a href="https://github.com/h0uter" rel="external nofollow noopener" target="_blank">Github</a>]</td> <td>Mats [<a href="https://github.com/1997rijkeboer" rel="external nofollow noopener" target="_blank">Github</a>]</td> </tr> </tbody> </table> <h1 id="mldg-algorithm">MLDG Algorithm</h1> <hr> <p>The meta learning algorithm used in the paper is designed to make the model more robust for domain shifts. Therefore, Domain Generalization methods are used. Hence, the algorithm is called Meta-Learning Domain Generalization. The high-level pseudocode of the algorithm can be found in Figure 1</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://i.imgur.com/VNZicw9.png" sizes="95vw"></source> <img src="https://i.imgur.com/VNZicw9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Pseudocode Algorithm MLDG </div> <p>As can be seen in line 2, the algorithm starts off by defining the domains S (i.e. Photo, Art painting, Cartoon, Sketch). Hereafter, the initial model parameters (Theta) and hyperparameters (Alpha, Beta, Gamma) are set. Line 4 denotes the start of the iterations. In each iteration, the training domain data are split in a meta-train set and a meta-test set as can be seen in Figure 2. It should be clear that the meta-test set is composed out of training data and not test data.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://i.imgur.com/shaKYDi.jpg" sizes="95vw"></source> <img src="https://i.imgur.com/shaKYDi.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Splitting the trainings domains </div> <p>Subsequently, the gradients for meta-train are calculated using the loss function (F). With this gradient, the proposed updated parameters can be calculated for the meta train set. Thus far, nothing new occurs except for splitting the domains in sets, when compared to normal backpropagation. However, in line 8 the loss function (G) is calculated for the meta-test set as well. In line 9, the updated parameters (Theta) are calculated based on the meta-train set loss function (F) and the meta-test set loss function (G) times a constant gamma. One can intuitively interpret this as the usual parameter calculation based on the train set gradients and loss function (F). However, these parameters are corrected by the loss function of the meta-test set (G). Therefore, the model will not overfit on a set of domains.</p> <h1 id="experiment-to-reproduce">Experiment to reproduce</h1> <hr> <p>From the paper, experiment 2 regarding object recognition had to be reproduced. The goal of this experiment is to recognize objects in one domain, while training the model in another. The goal is to obtain a domain-invariant feature representation.</p> <p>For this experiment, the PACS multi-domain recognition benchmark was used. This dataset is designed specifically for cross-domain recognition problems [Li et al. 2017]. The dataset contains 9991 images across 7 different categories spread over 4 domains. These categories and domains are listed below.</p> <h3 id="categories">Categories</h3> <ul> <li>Dog</li> <li>Elephant</li> <li>Giraffe</li> <li>Guitar</li> <li>House</li> <li>Horse</li> <li>Person</li> </ul> <h3 id="domains">Domains</h3> <ul> <li>Photo</li> <li>Art painting</li> <li>Cartoon</li> <li>Sketch</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://i.imgur.com/XOmolUK.png" sizes="95vw"></source> <img src="https://i.imgur.com/XOmolUK.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3: PACS (Domains S) </div> <p>The proposed MLDG algorithm is compared against 4 baseline models. These are shown below.</p> <h3 id="baselines">Baselines</h3> <ul> <li>D-MTAE</li> <li>Deep-all</li> <li>DSN</li> <li>AlexNet+TF</li> </ul> <p>The results, which are to be reproduced, are shown in table 1. In this table, the accuracy of the considered baselines on the four different domains are shown as well as the results for the proposed MLDG algorithm. The goal of this reproducability project is to reproduce the accuracy of the proposed MLDG algorithm on the different domains (right column of table 1).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://i.imgur.com/80yZ6ih.png" sizes="95vw"></source> <img src="https://i.imgur.com/80yZ6ih.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="reproduction">Reproduction</h1> <hr> <p>The authors of the paper have published the code used for the first draft of their paper. This publicly available repository will be used as a starting point for the reproducability project.</p> <h2 id="understanding-the-code">Understanding the code</h2> <p>There are 4 main files</p> <ol> <li><code class="language-plaintext highlighter-rouge">main_baseline.py</code></li> <li><code class="language-plaintext highlighter-rouge">main_mldg.py</code></li> <li><code class="language-plaintext highlighter-rouge">model.py</code></li> <li><code class="language-plaintext highlighter-rouge">MLP.py</code></li> </ol> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://i.imgur.com/N82CWi8.jpg" sizes="95vw"></source> <img src="https://i.imgur.com/N82CWi8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Code Dependencies </div> <p>Lets start at the end and then walk our way backwards. The dependencies of the python files are as shown in figure 4.</p> <p>The networks are ran from their respective <code class="language-plaintext highlighter-rouge">run_XXX.sh</code> run files. Here the hyper parameters are specified.</p> <p>For MLDG only:</p> <ul> <li>step size of the meta-learn step: meta_step_size</li> <li>value for the hyper parameter beta: meta_val_beta</li> </ul> <p>The models are defined in main_XXX.py by initialising their respective classes with a long list of (hyper) parameters. These parameters include:</p> <ul> <li>number of test every steps: <code class="language-plaintext highlighter-rouge">test_every</code> </li> <li>batch size for training <code class="language-plaintext highlighter-rouge">batch_size</code> (default is 64)</li> <li>number of classes: <code class="language-plaintext highlighter-rouge">num_classes</code> </li> <li>momentum: <code class="language-plaintext highlighter-rouge">momentum</code> </li> <li>number of classes” <code class="language-plaintext highlighter-rouge">inner_loops</code> </li> <li>number of step size to decay the lr: <code class="language-plaintext highlighter-rouge">step_size</code> </li> <li>index of unseen domain: <code class="language-plaintext highlighter-rouge">unseen_index</code> </li> <li>learning rate of the model: <code class="language-plaintext highlighter-rouge">lr</code> </li> <li>weight decay: <code class="language-plaintext highlighter-rouge">weight_decay</code> </li> </ul> <p>Both classes are defined in <code class="language-plaintext highlighter-rouge">model.py</code>. The MLDG model defines a new train method and inherits all other methods and properties from the baseline (MLP) model.</p> <p>We will now discuss the interesting parts of the MLDG train method. We first start by calculating the meta-training loss.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">images_train</span><span class="p">,</span> <span class="n">labels_train</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">batImageGenTrains</span><span class="p">[</span><span class="n">index</span><span class="p">].</span><span class="nf">get_images_labels_batch</span><span class="p">()</span>

<span class="n">inputs_train</span><span class="p">,</span> <span class="n">labels_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">images_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">labels_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>

<span class="c1"># wrap the inputs and labels in Variable
</span><span class="n">inputs_train</span><span class="p">,</span> <span class="n">labels_train</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">inputs_train</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">cuda</span><span class="p">(),</span> \
    <span class="nc">Variable</span><span class="p">(</span><span class="n">labels_train</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">long</span><span class="p">().</span><span class="nf">cuda</span><span class="p">()</span>

</code></pre></div></div> <p>As can be seen in the code above, the numpy data structures are converted to Torch compatible data structures and subsequently converted to CUDA variables.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># forward with the adapted parameters
</span><span class="n">outputs_train</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">inputs_train</span><span class="p">)</span>

<span class="c1"># loss
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">loss_fn</span><span class="p">(</span><span class="n">outputs_train</span><span class="p">,</span> <span class="n">labels_train</span><span class="p">)</span>
<span class="n">meta_train_loss</span> <span class="o">+=</span> <span class="n">loss</span>
</code></pre></div></div> <p>Then a regular forward pass is executed and the loss is calculated.</p> <p>Next, the meta-validation loss is calculated. This can be seen in the code below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_val</span><span class="p">,</span> <span class="n">labels_val</span> <span class="o">=</span> <span class="n">batImageMetaVal</span><span class="p">.</span><span class="nf">get_images_labels_batch</span><span class="p">()</span>
            <span class="n">inputs_val</span><span class="p">,</span> <span class="n">labels_val</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span>
                <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">image_val</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span>
                <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">labels_val</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>

<span class="c1"># wrap the inputs and labels in Variable
</span><span class="n">inputs_val</span><span class="p">,</span> <span class="n">labels_val</span> <span class="o">=</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">inputs_val</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">cuda</span><span class="p">(),</span> \
    <span class="nc">Variable</span><span class="p">(</span><span class="n">labels_val</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">long</span><span class="p">().</span><span class="nf">cuda</span><span class="p">()</span>

<span class="c1"># forward with the adapted parameters
</span><span class="n">outputs_val</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">inputs_val</span><span class="p">,</span>
                              <span class="n">meta_loss</span><span class="o">=</span><span class="n">meta_train_loss</span><span class="p">,</span>
                              <span class="n">meta_step_size</span><span class="o">=</span><span class="n">flags</span><span class="p">.</span><span class="n">meta_step_size</span><span class="p">,</span>
                              <span class="n">stop_gradient</span><span class="o">=</span><span class="n">flags</span><span class="p">.</span><span class="n">stop_gradient</span><span class="p">)</span>

<span class="n">meta_val_loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">loss_fn</span><span class="p">(</span><span class="n">outputs_val</span><span class="p">,</span> <span class="n">labels_val</span><span class="p">)</span>
</code></pre></div></div> <p>Now we essentially do the same as above but this time the proposed adapted parameters are tested on the meta-test domains. This is the key strategy of this method. The proposed parameters are only desirable if they lead to both increased performance on the meta-train domain as well as on the meta-test domain. More specifically we are looking for adaptations which lead to increased performance across a whole range of very different domains (for example PACS, as shown before).</p> <p>Here the hyper parameter of meta-stepsize has also come into play. The meta-stepsize governs the step size during the meta-training whereas the regular step size governs actual train step.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>total_loss = meta_train_loss + meta_val_loss * flags.meta_val_beta
</code></pre></div></div> <p>Now for training the <code class="language-plaintext highlighter-rouge">total_loss</code> is used. The total loss is composed of the meta-train loss and the meta-test loss. The <code class="language-plaintext highlighter-rouge">meta_val_beta</code> hyper parameter is used to tune the relative importance of both. Finally this loss is used in a regular backpropagation step with Stochastic Gradient Descent.</p> <h2 id="efforts">Efforts</h2> <p>First we dived into the existing code of the authors. After inspection, it was found that it was written in Python 2 and made hefty use of numpy instead of readily available PyTorch methods.</p> <p>Our reproduction efforts are summarized below</p> <ul> <li> <strong>Rewrite to Python 3</strong><br> As of January 2020, Python 2 will receive the EOL (End Of Life) status and will no longer receive official support. To comply with best practices, the original codebase was rewritten to Python 3.</li> <li> <strong>Get code running on Google Colab</strong><br> Google Colab was used to deploy and run the repository. The entrypoint to the original codebase are the files <code class="language-plaintext highlighter-rouge">run_baseline.sh</code> and <code class="language-plaintext highlighter-rouge">run_mldg.sh</code>. To avoid extra work in order to run shell scrips in Colab, these entrypoints were rewritten to Python 3 files named <code class="language-plaintext highlighter-rouge">run_baseline.py</code> and <code class="language-plaintext highlighter-rouge">run_mldg.py</code> respectively.</li> <li> <strong>Loop unrolling</strong><br> The first challenge we encountered was that google Colab stopped running too early to complete multiple runs back-to-back. Each run consist of 4 iterations in which a single domain is held out from the training set. To solve this problem, the outer loop of the entry scripts is unrolled, enabling the runs to be executed independently.</li> <li> <p><strong>Hyperparameter investigation</strong><br> Because the code is available for a given paper does not necessarily mean that it is independently reproducible. To get a better feel for the independent reproducibility, it is investigated whether all hyperparameters used for this experiment are specified in the paper. This can be seen in below in table 2.</p> <table> <thead> <tr> <th>Hyperparameter</th> <th>Value</th> <th>Specified in paper</th> </tr> </thead> <tbody> <tr> <td>Learning rate</td> <td>5e-4</td> <td>Yes</td> </tr> <tr> <td>Decay step</td> <td>15 000</td> <td>Yes</td> </tr> <tr> <td>Decay rate</td> <td>0.96</td> <td>Yes</td> </tr> <tr> <td>Batch size</td> <td>64</td> <td>Yes</td> </tr> <tr> <td>Validation beta</td> <td>1.0</td> <td>Yes</td> </tr> <tr> <td>Meta step size</td> <td>5e-4</td> <td>Yes</td> </tr> <tr> <td>Iterations</td> <td>15 000</td> <td>Yes</td> </tr> <tr> <td>Test_every</td> <td>500</td> <td>No</td> </tr> </tbody> </table> <p><em>Table 2: Hyperparameter Investigation</em></p> <p>From these hyperparameters, only <code class="language-plaintext highlighter-rouge">Test_every</code> was defined in the original code of the authors, but not in the paper. This parameter sets the number of training iterations before the network is tested on the validation set in order to log the intermediate accuracy. This parameter has no influence over the training of the network and therefore has no influence on the accuracy on the held out test.</p> </li> </ul> <p>Apart from the above mentioned efforts, a significant effort went into fully understanding the code and correctly interpreting the obtained results.</p> <h2 id="results">Results</h2> <p>Using the code as described above (see <a href="https://github.com/h0uter/MLDG/tree/python3" rel="external nofollow noopener" target="_blank">here</a> for the full repository), an attempt was made to reproduce the results from table 1. A baseline (MLP) and the MLDG model are trained on the extracted features of the PACS dataset. The accuracy of both models for the four different domains can be seen below in table 3.</p> <table> <thead> <tr> <th>Domain</th> <th>MLP</th> <th>MLDG</th> </tr> </thead> <tbody> <tr> <td>art_painting</td> <td>69.27</td> <td>71.75</td> </tr> <tr> <td>cartoon</td> <td>52.01</td> <td>48.30</td> </tr> <tr> <td>photo</td> <td>89.61</td> <td>95.74</td> </tr> <tr> <td>sketch</td> <td>33.72</td> <td>38.94</td> </tr> <tr> <td>Ave.</td> <td>61.15</td> <td>63.68</td> </tr> </tbody> </table> <p><em>Table 3: Reproduced Results for MLP and MLDG</em></p> <p>These results were obtained by performing three independent runs and averaging the resulting accuracy per domain.</p> <p>The comparison of the reproduced MLDG results against the original MLDG results from table 1 can be seen in table 4.</p> <table> <thead> <tr> <th>Domain</th> <th>Original</th> <th>Reproduced</th> <th>Difference</th> </tr> </thead> <tbody> <tr> <td>art_painting</td> <td>66.23</td> <td>71.75</td> <td>+8.33 %</td> </tr> <tr> <td>cartoon</td> <td>66.88</td> <td>48.30</td> <td>-27.8 %</td> </tr> <tr> <td>photo</td> <td>88.00</td> <td>95.74</td> <td>+8.79 %</td> </tr> <tr> <td>sketch</td> <td>57.51</td> <td>38.94</td> <td>-32.3 %</td> </tr> <tr> <td>Ave.</td> <td>70.01</td> <td>63.68</td> <td>-9.04 %</td> </tr> </tbody> </table> <p><em>Table 4: Comparison Original Results and Reproduced Results</em></p> <p>It can be argued that these reproduced accuracies improve when more independent runs are performed. In order to investigate this, a 95% confidence interval is calculated for the average accuracy of the reproduced MLDG model. This confidence interval is given by</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.637, 95% CI [0.635, 0.639]
</code></pre></div></div> <h3 id="investigating-meta-losses">Investigating meta losses</h3> <p>Apart from reproducing the results in table 1, the meta train loss and meta validation loss were investigated as a function of iteration in order to get a better understanding of the MLDG algorithm.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://i.imgur.com/Xa9S5Wx.png" sizes="95vw"></source> <img src="https://i.imgur.com/Xa9S5Wx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5: Meta Losses Training on Photo Domain </div> <p>From figure 5, it seems as if the plot might have benefitted from a more decaying step size to prevent large fluctuations. Similarly, we could speculate that the validation set used here is unrepresentative [Machine learning mastery, Apr 2020]. This makes sense, as it is not composed of all domains. In this case it indicates that the validation dataset may be easier for the model to predict than the training dataset. This can be seen from the lower average losses for the validation set compared to the training set. The domain in the validation set is therefore better predictable than the ones in the training set. This is of course logical due to the model not performing completely equally for all domains, which can be seen in previous tables. Also, the model doesn’t seem to be overfit and no early-stop has to be performed. The difference between the two lines denotes the generalization error.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://i.imgur.com/Wlvdgdr.png" sizes="95vw"></source> <img src="https://i.imgur.com/Wlvdgdr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6: Meta Losses Training on Sketch Domain </div> <p>From figure 6 can be seen that after iteration 120 little improvement is made. To compare, in Figure: Photo Domain it seems as if after iteration 250 the model doesn’t learn a lot. Also, the generalisation is very small in the sketch domain plot. It seems like a good fit model. However as could be seen from table 3, the accuracy on this domain is rather low. The validation losses are small, indicating that model is complex enough for the validation set. The training losses are also small and stable, indicating low variance. It might be a possibility that the model is overfitted on the training domains (meta-training set + meta-test/ validation set), but that this does not generalize well to the sketch domain.</p> <h1 id="conclusion">Conclusion</h1> <p>The first thing to notice is that the reproduced baseline model (MLP) is different from the baseline models in table 1. This is not vital for our reproducibility project since the baseline results are not part of our reproducibility goal (right most column in table 1). However, as the model is different, no clear comparison between the baselines can be made. The MLP baseline can however be used to compare against the reproduced MLDG results.</p> <p>By comparing tables 1 and 3, it becomes clear that domains, which were considered easy or difficult to meta learn, remain the same in the reproduction. The sketch domain receives the lowest accuracy scores in both table 1 and 3, while the photo domain receives the highest scores.</p> <p>While the ranking of easy and difficult domains to meta learn remain the same in the reproduction project, the achieved accuracy does not. In table 4 it can be seen that the accuracy of the reproduced MLDG model on the different domain is significantly different from the original results. On average, the reproduced accuracy across all domains is 9.04% lower than the original results. In the extreme case of the sketch domain, the reproduced results were 32.3% lower.</p> <p>The uncertainty of these reproduced results is also investigated by means of a confidence interval. It can be seen that the original accuracy across all domains does not lie within the 95% CI [0.635, 0.639]. Therefore, it can be concluded that it is unlikely that these differences in accuracy are due to uncertainty in the reproduced accuracies.</p> <p>Because the rankings of the domains in terms of difficulty stays more or less the same, there seems to be a systematic difference between the reproduced results and the original results. A random error is ruled out by the 95% confidence interval.</p> <p>To conclude, the original results obtained in the paper were not reproduced.</p> <h1 id="discussion">Discussion</h1> <p>There could be multiple reasons why the original results from table 1 were not fully reproducible.</p> <p>A possible explaination could be that the authors slightly altered/optimized their algorithm while rewriting it to Tensorflow. The available code base is a previous version of the code, which after inspection and testing should behave exactly like the pseudocode algorithm in figure 1. It could be possible that the overall algorithm was altered or optimized in some way when it was rewritten into Tensorflow.</p> <p>A second possibility could be that the number of runs performed is critical to the accuracy of the original paper. In our reproducibility project, only 3 runs were performed because it was found that the accuracy did not vary significantly between runs (see the 95% confidence interval). It could be that the authors ran the experiment numerous times and averaged the resulting accuracies, but this was not specified in the paper.</p> <p>Whether or not this is the case could be investigated in future work.</p> <h3 id="future-work"><strong>Future Work</strong></h3> <p>In order to give a better insight into the independent reproducability of the paper, it could be an option to completely rewrite the codebase based on the pseudocode algorithm from figure 1 and the hyperparameters specified in the paper.</p> <p>If this produces similar results to the reproduced results in table 3 it could be concluded that this paper is not independently reproducible.</p> <h2 id="references">References</h2> <ul> <li>[Li et al. 2017] Li, D.; Yang, Y.; Song, Y.-Z.; and Hospedales, T. 2017. Deeper, broader and artier domain generalisation. In ICCV.</li> <li>[Machine learning mastery, Apr 2020] https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/</li> </ul> <hr> <p>Part of <a href="https://reproducedpapers.org/" rel="external nofollow noopener" target="_blank">DL Reproducability Project</a></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Wouter J. Meijer. Last updated: March 28, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"projects",description:"A growing collection of awesome projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"publications",description:"publications in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"dropdown-tools",title:"tools",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-resources",title:"resources",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"news-i-will-start-a-new-challenge-as-a-robotics-researcher-at-tno-within-the-intelligent-autonomous-systems-group",title:"I will start a new challenge as a Robotics Researcher at TNO within...",description:"",section:"News"},{id:"news-we-got-our-paper-accepted-at-icra-so-i-m-going-to-london",title:"We got our paper accepted at ICRA, so I\u2019m going to London!",description:"",section:"News"},{id:"news-we-got-papers-accepted-at-icra-and-icprai-i-will-get-the-opportunity-to-present-them-in-japan-and-korea-respectively",title:"We got papers accepted at ICRA and ICPRAI, I will get the opportunity...",description:"",section:"News"},{id:"news-we-got-2-papers-accepted-at-rss-right-here-in-delft",title:"We got 2 papers accepted at RSS, right here in Delft!",description:"",section:"News"},{id:"projects-upgrading-car-radio",title:"Upgrading Car Radio",description:"&quot;Spotify take the wheel!&quot; with a custom 3.5mm jack.",section:"Projects",handler:()=>{window.location.href="/projects/2017-03-11=car-aux-project/"}},{id:"projects-skelex-at-the-hannover-messe",title:"Skelex at the Hannover-Messe",description:"Business development for an international audience.",section:"Projects",handler:()=>{window.location.href="/projects/2019-04-02=Hannover-Messe-Skelex/"}},{id:"projects-roi-calculator",title:"ROI Calculator",description:"A calculator which gives insight in the financial benefits of upgrading to LED lights in office buildings.",section:"Projects",handler:()=>{window.location.href="/projects/2019-04-03=LED-ROI/"}},{id:"projects-pendulum-control",title:"Pendulum Control",description:"Various controller designs for double pendulum.",section:"Projects",handler:()=>{window.location.href="/projects/2020-03-12=CSD-double-pendulum/"}},{id:"projects-take-a-seat",title:"Take a Seat",description:"Office sharing to connect small and large businesses.",section:"Projects",handler:()=>{window.location.href="/projects/2020-04-03=Take-a-seat/"}},{id:"projects-mldg-reproduction",title:"MLDG reproduction",description:"Meta Learning for Domain Generalisation enables us to train on photos and then classify drawings.",section:"Projects",handler:()=>{window.location.href="/projects/2020-04-20=MLDG-project/"}},{id:"projects-cutlery-classifier",title:"Cutlery Classifier",description:"End-to-end ML pipeline for robot gripper.",section:"Projects",handler:()=>{window.location.href="/projects/2020-10-25=ml-for-robotics-project/"}},{id:"projects-3d-sensor-data-fusion",title:"3D Sensor Data Fusion",description:"Combining LiDAR and stereo camera pointclouds to build an occupancy grid for navigating an autonomous vehicle.",section:"Projects",handler:()=>{window.location.href="/projects/2021-01-08=machine-perception-project/"}},{id:"projects-quadrotor-motion-planning",title:"Quadrotor Motion Planning",description:"Expanding upon RRT* with a line-of-sight check.",section:"Projects",handler:()=>{window.location.href="/projects/2021-01-08=quadrotor-mp-project/"}},{id:"projects-ai-for-retail",title:"AI for retail",description:"Utilizing formal knowledge representation and reasoning methods to stock shelves in the supermarket.",section:"Projects",handler:()=>{window.location.href="/projects/2021-03-16=KRR/"}},{id:"projects-pilco-reproduction",title:"PILCO Reproduction",description:"Data efficient reinforcement learning with gaussian processes.",section:"Projects",handler:()=>{window.location.href="/projects/2021-03-17=Data-Efficient-Reinforcement-Learning-Project/"}},{id:"projects-thesis",title:"Thesis",description:"Situational Graphs for Task-Oriented Mapping on Mobile Robots",section:"Projects",handler:()=>{window.location.href="/projects/2022-05-29=thesis/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%6F%75%74%65%72%6D+%70%6F%72%74%66%6F%6C%69%6F@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=jGEettwAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/h0uter","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/wouterjmeijer","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>